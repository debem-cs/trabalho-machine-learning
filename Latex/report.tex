\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% Margins setup for a standard academic report
\geometry{
 a4paper,
 left=25mm,
 top=25mm,
 bottom=25mm,
 right=25mm
}

% Title Format
\title{\textbf{Change Detection in Satellite Imagery: \\ Feature Engineering and Model Optimization Report}}
\author{\textbf{Team: 3 Mosqueteiros} \\
Enzo Jardim Vendramin \\
Matheus Carvalho da Costa \\
Pedro de Bem e Canto Cantanhede}
\date{\today}

\begin{document}

\maketitle

\section{Feature Engineering}

The primary challenge in this dataset involves distinguishing genuine land-use changes from natural variations such as seasonal phenology or lighting differences. Our feature engineering strategy was therefore driven by the motivation to capture the \textit{dynamics} of the spectral signatures rather than relying solely on static snapshots.

\subsection{Capturing Temporal Dynamics}
The cornerstone of our approach was the construction of features that explicitly model the evolution of a polygon's spectral properties over time. We hypothesized that a change in land use, such as the construction of a building or a road, would manifest as a significant and abrupt shift in the Red, Green, and Blue bands. To capture this, we calculated the temporal differences between consecutive time steps for all available spectral bands. For instance, computing the difference between the mean red value at date $t+1$ and date $t$ allows the model to detect the magnitude and direction of spectral change. 

However, absolute differences can be misleading due to varying lighting conditions across different satellite passes. To mitigate this, we also derived relative change ratios. By normalizing the difference by the initial value, we provided the classifier with a metric representing the percentage change, which is more robust to illumination artifacts. Furthermore, we aggregated these temporal signals by calculating global statistics—specifically the standard deviation and the range (max-min)—across all time points for each polygon. A high temporal standard deviation serves as a strong proxy for instability, flagging polygons that are likely undergoing transformation, whereas low variance suggests a stable land class such as an established forest or water body.

\subsection{Addressing Seasonality and Phenology}
A significant source of noise in change detection is the natural seasonal cycle of vegetation. A polygon might appear different in winter compared to summer solely due to phenological changes, not because of actual construction. To prevent the model from misinterpreting these seasonal shifts as true structural changes, we extracted temporal metadata from the provided dates. Instead of using raw dates which can be sparse, we decomposed the dates into Month and Day-of-Year components.

Crucially, we applied a cyclical encoding to the Day-of-Year variable using Sine and Cosine transformations. This ensures that the inputs respect the circular nature of time, where the end of one year is adjacent to the beginning of the next. This feature allows the decision trees to learn seasonal context, effectively ``adjusting'' their expectations of spectral values based on the time of year, thereby isolating the anomaly of true change from the background signal of seasonal variation.

\subsection{Geometry and Categorical Embeddings}
Beyond temporal signals, the physical shape of the polygon provides valuable context. Man-made structures often exhibit regular geometric properties compared to natural features. We computed the Area, Perimeter, and Compactness for each polygon. Compactness, defined as the ratio of area to the square of the perimeter, helps distinguish between compact shapes (like buildings) and elongated shapes (like roads or rivers). Finally, categorical variables such as \texttt{urban\_type} and \texttt{geography\_type} were One-Hot Encoded. This was necessary as these categories do not possess an inherent ordinal relationship, and this encoding allows the tree-based models to split effectively on specific urban contexts without imposing an artificial hierarchy.

\section{Model Tuning and Comparison}

Our modeling strategy prioritized robustness and generalization capability. We focused on tree-based ensemble methods, specifically Random Forest and XGBoost, due to their ability to handle non-linear interactions and input scales without extensive normalization.

\subsection{Strategic Focus on Dominant Classes}
Given the severe class imbalance in the dataset—where \textit{Residential} and \textit{Commercial} classes comprise over 84\% of the data—we adopted a strategy to maximize the Weighted F1-Score. Unlike Macro F1, which treats all classes equally, Weighted F1 is driven by performance on the majority classes. Consequently, we prioritized models that achieved high accuracy on these dominant categories. We employed a Standard Random Forest (without class balancing) and tuned our XGBoost objective specifically to maximize this frequency-weighted metric. This approach effectively aligned our model's objective with the global distribution of the test data.

\subsection{Hyperparameter Tuning Procedure}
We employed \texttt{RandomizedSearchCV} to tune the XGBoost classifier, favoring it over Grid Search to explore a wider range of the hyperparameter space efficiently. Key parameters included the learning rate, tree depth, and subsampling ratios. We found that a lower learning rate (0.01) combined with a larger number of estimators (300) significantly improved generalization by ensuring the model learned the residuals gradually. Additionally, a maximum tree depth of 10 was selected. While deeper trees can capture more complex patterns, they risk overfitting. We balanced this by setting the subsample and colsample\_bytree parameters to 0.8, introducing stochasticity that further regularizes the model and prevents it from relying too heavily on any single feature or data point.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{feature_importance.png}
\caption{Feature Weights Optimized for Maximizing Accuracy.}
    \label{fig:feature_importance}
\end{figure}

\subsection{Ensemble Strategy for Generalization}
To ensure the final solution was robust and not overly reliant on the biases of a single algorithm, we implemented a Voting Ensemble. We combined the predictions of the tuned XGBoost model and the Standard Random Forest using a weighted average of their predicted probabilities. 
 
This ensemble strategy leverages the diverse strengths of the two algorithms: Random Forest reduces variance through bagging and random feature selection, while XGBoost reduces bias through boosting. The final model weights, approximately equal contributions, were determined based on their individual cross-validated performance. This approach yielded a consistent local Weighted F1-Score of approximately 0.78, which translated to a robust performance of approximately 0.93 on the unseen test data.

\subsection{Advanced Optimization: Pseudo-Labeling}
To further push the model's performance, we implemented a semi-supervised learning technique known as Pseudo-Labeling. This process involves generating predictions on the test set using our initial ensemble. We then filtered these predictions to retain only those samples where the model's confidence probability exceeded a strict threshold of 95\%. These high-confidence test samples were treated as ground truth and appended to the original training dataset. By retraining the ensemble on this augmented dataset, the model was able to adapt to the specific distribution of the test data, effectively densifying the decision boundaries in regions that were previously sparse. This strategy aimed to capture edge cases that were underrepresented in the original labeled data.

\end{document}
